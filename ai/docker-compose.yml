services:
  n8n:
    stdin_open: true
    tty: true
    container_name: n8n
    image: docker.bigmaninc.party/docker/n8nio/n8n:stable
    pull_policy: always
    environment:
      - N8N_LISTEN_ADDRESS=0.0.0.0
      - N8N_PORT=5678
      - N8N_SECURE_COOKIE=false
      - DOCKER_PULL=always
    volumes:
      - n8n_data:/home/node/.n8n
    restart: unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.services.n8n.loadbalancer.server.port=5678
      - traefik.http.routers.n8n.rule=Host(`n8n.bigmaninc.party`)
      - traefik.http.routers.n8n.entrypoints=web
      - traefik.http.routers.n8n.service=n8n
      - traefik.http.routers.n8n.middlewares=ts-only

    networks:
      - homelab_proxy

  ollama:
    image: docker.bigmaninc.party/docker/ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks:
      - ai_internal
    volumes:
      - ollama_data:/root/.ollama
    environment:
      # keep default 11434 inside the docker network
      - OLLAMA_HOST=0.0.0.0:11434
    # optional GPU (only works on Docker hosts configured for it)
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: ${OLLAMA_GPU_COUNT-1}
              capabilities: [gpu]
    # gpu: all

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-cuda}
    container_name: open-webui
    restart: unless-stopped
    depends_on:
      - ollama
    networks:
      - homelab_proxy
      - ai_internal
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_URL=${WEBUI_URL}
      - ENABLE_SIGNUP=${ENABLE_SIGNUP-false}
    labels:
      - "traefik.enable=true"
      - "traefik.http.services.openwebui.loadbalancer.server.port=8080"
      - "traefik.http.routers.openwebui.rule=Host(`chat.bigmaninc.party`)"
      - "traefik.http.routers.openwebui.entrypoints=web"
      - "traefik.http.routers.openwebui.service=openwebui"
    gpus: all

volumes:
  n8n_data: {}
  ollama_data: {}
  open-webui: {}

networks:
  homelab_proxy:
    external: true
  ai_internal:
    internal: true